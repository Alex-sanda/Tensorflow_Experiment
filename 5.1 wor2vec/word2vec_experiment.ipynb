{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf \n",
    "import random\n",
    "import urllib\n",
    "import zipfile\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 : process data\n",
    "#1.1下载数据\n",
    "#1.2读取数据\n",
    "#1.3构建字典\n",
    "#1.4将词转换为下标\n",
    "#1.5按照skip-gram重新组织数据\n",
    "#1.6把一个batch数据处理成numpy格式返回\n",
    "#集成 ： 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.1 下载数据\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'#下载地址\n",
    "FILE_NAME = 'text8.zip'#文件名\n",
    "DATA_FOLDER = 'data/'#存放路径\n",
    "EXPECTED_BYTES = 31344016\n",
    "\n",
    "#数据预处理参数\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "NUM_SAMPLED = 64    # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "WEIGHTS_FLD = 'processed/'\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "def download_data(file_name,from_url,to_floder,expected_bytes):\n",
    "    download_from_path = from_url + file_name    #下载路径\n",
    "    save_to_path = to_floder + file_name         #存储路径\n",
    "    if os.path.exists(save_to_path):             #检查文件是否已经下载\n",
    "        print(\"Data Ready : Data has Already been download\")\n",
    "        return save_to_path\n",
    "    print(\"Down load Data,please wait\")\n",
    "    file_path,_ = urllib.request.urlretrieve(download_from_path,filename=save_to_path)\n",
    "    \n",
    "    #验证文件已经下载完成\n",
    "    if os.stat('data/text8.zip').st_size == EXPECTED_BYTES :\n",
    "        print(\"Data Ready : Down load complete\")\n",
    "    else:\n",
    "        raise Exception('File'+file_name+\"might have been corrupted,download again\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.2 读取数据\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile(DATA_FOLDER+FILE_NAME) as file:\n",
    "        doc_str=\"\"\n",
    "        for f_name in file.namelist():\n",
    "            file_str = file.read(f_name)\n",
    "            #print(\"FileName :\",f_name,\";    [:20]:\",file_str[:20])\n",
    "            doc_str+=file_str.decode()\n",
    "        words = tf.compat.as_str(doc_str).split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.3 构建字典\n",
    "#为了节省内存，限制字典的大小\n",
    "#因此将出现频率最高的词抽取出来构建字典\n",
    "#vocab_size = 1000\n",
    "def build_word2vec_dict(words,vocab_size):\n",
    "    counted = Counter(words) #构建Counter对象\n",
    "    words_most_common = [('UNK',-1)]\n",
    "    words_most_common.extend(counted.most_common(n=vocab_size-1))#抽取高频词汇，输出一个list\n",
    "                                                                    #形式[(word_1,112),(word_2,111)...]\n",
    "    word2vec_dictionary ={}#构建字典\n",
    "    with open('processed/vocab_1000.tsv','w') as f: #部分words写入文档\n",
    "        index = 0\n",
    "        for word,_ in words_most_common:\n",
    "            word2vec_dictionary[word]=index\n",
    "            if index<1000:\n",
    "                f.write(word+\"\\n\")\n",
    "            index += 1\n",
    "        index_dict_for_word2vec = dict(zip(word2vec_dictionary.values(),word2vec_dictionary.keys()))\n",
    "    return word2vec_dictionary,index_dict_for_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.4 将词转换为下标\n",
    "def convert_words_to_index(words,vocab_dict):\n",
    "    return [vocab_dict[word] if word in vocab_dict else 0 for word in words]\n",
    "#converted_words = convert_words_to_index(words,word2vec_dictionary)\n",
    "#len(converted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#1.5 按照skip-gram重新组织数据\n",
    "\n",
    "\n",
    "为什么窗口随机? : -https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_example(converted_words,skip_window=SKIP_WINDOW):#可迭代，每次生成一个example\n",
    "    window_size = random.randint(1,skip_window)\n",
    "    for center_index,center_word in enumerate(converted_words):  #抽取中心词\n",
    "        #从中心词左边抽取，生成example\n",
    "        for word in converted_words[max(0,center_index-window_size):center_index]:\n",
    "            yield center_word,word               #,center_index,\"left\"\n",
    "        #从中心词右边抽取，生成example\n",
    "        for word in converted_words[center_index + 1 : min(center_index+window_size+1,len(converted_words)-1)]:\n",
    "            yield center_word,word               #,center_index,\"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def example_batch_producer(raw_data,method_generate_example,batch_size,skip_window=SKIP_WINDOW): #可迭代,每次生成一个batch\n",
    "    example_iterator = method_generate_example(raw_data,skip_window)\n",
    "    while True:\n",
    "        batch_data_list = []\n",
    "        for i in range(batch_size):\n",
    "            batch_data_list.append(next(example_iterator))\n",
    "        example_batch = np.array(batch_data_list)\n",
    "        yield example_batch\n",
    "\n",
    "#atch_example_iterator = example_batch_producer(converted_words,get_example,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.6 数据处理\n",
    "def process_data(vocab_size=VOCAB_SIZE,batch_size=BATCH_SIZE,skip_window=SKIP_WINDOW):\n",
    "    \"\"\"Return a iterator that generate example_batch\n",
    "        each batch is [\n",
    "        [center_word,target_word], #in index num\n",
    "        [center_word,target_word]\n",
    "        [center_word,target_word]\n",
    "        ... ...\n",
    "        ]\n",
    "\n",
    "\n",
    "    Example\n",
    "    example_batch_generator = process_data()\n",
    "    for i in range(5):\n",
    "        print(next(example_batch_generator).shape)\n",
    "    \"\"\" \n",
    "    data_path = download_data(FILE_NAME,DOWNLOAD_URL,DATA_FOLDER,None)#下载数据，返回存储路径\n",
    "    docs = read_data(data_path)\n",
    "    word2vec_dictionary,index_dict_for_word2vec = build_word2vec_dict(docs,vocab_size)\n",
    "    converted_docs = convert_words_to_index(docs,word2vec_dictionary)\n",
    "    del docs#节省空间\n",
    "    batch_example_iterator = example_batch_producer(converted_docs,get_example,batch_size)\n",
    "    return batch_example_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2 : build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel(object):\n",
    "    def __init__(self,vocab_size,embed_size,batch_size,num_sample,learning_rate):\n",
    "        self.name = \"Skip Gram Model\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sample = num_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        \"\"\" 定义place_holder作为数据入口 \"\"\"\n",
    "        with tf.variable_scope(\"data\"):\n",
    "            self.center_words_batch = tf.placeholder(tf.int32,shape=[self.batch_size],name=\"center_words\")\n",
    "            self.target_words_batch = tf.placeholder(tf.int32,shape=[self.batch_size,1],name=\"target_words\")\n",
    "    \n",
    "    \"\"\" 定义权重矩阵(注意word2vec里实际上这个权重矩阵里，每一行(或者列)就是我们的词向量，所以很重要) \"\"\"\n",
    "    def _create_embedding(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope(\"embed\"):\n",
    "                self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size,self.embed_size],-1.0,1.0),name=\"embed_matrix\")\n",
    "                \n",
    "    \"\"\" 定义word2vec结果，同时定义损失函数 \"\"\"\n",
    "    def _get_loss(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                embed = tf.nn.embedding_lookup(self.embed_matrix,self.center_words_batch)\n",
    "                soft_max_weight = tf.Variable(tf.truncated_normal([self.vocab_size,self.embed_size],stddev=1.0/(self.embed_size**0.5)),name='nce_weight')\n",
    "                soft_max_bias   = tf.Variable(tf.zeros([self.vocab_size]),name='nce_bias')\n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(soft_max_weight,\n",
    "                                                          soft_max_bias,\n",
    "                                                          self.target_words_batch,\n",
    "                                                          embed,\n",
    "                                                          self.num_sample,\n",
    "                                                          self.vocab_size,\n",
    "                                                          name='loss')\n",
    "                                                         )\n",
    "    \n",
    "    \"\"\" 设定optimizer \"\"\"\n",
    "    def _create_optimizer(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss,global_step=self.global_step)\n",
    "    \n",
    "    \"\"\" 设定summary，以便在Tensorboard里进行可视化 \"\"\"\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "            tf.summary.histogram('histogram_loss',self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    \"\"\" 构建整个图的Graph \"\"\"\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholder()\n",
    "        self._create_embedding()\n",
    "        self._get_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_gram = SkipGramModel(VOCAB_SIZE,EMBED_SIZE,BATCH_SIZE,NUM_SAMPLED,LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ready : Data has Already been download\n"
     ]
    }
   ],
   "source": [
    "example_batch_generator = process_data()\n",
    "skip_gram._build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/skip-gram-67999\n",
      "Average_loss at step 69999 : 4.9190473415255545\n",
      "Average_loss at step 71999 : 4.909809581637383\n",
      "Average_loss at step 73999 : 4.84408086168766\n",
      "Average_loss at step 75999 : 4.857043523907661\n",
      "Average_loss at step 77999 : 4.793568802595138\n",
      "Average_loss at step 79999 : 4.849250820279122\n",
      "Average_loss at step 81999 : 4.657039531826973\n",
      "Average_loss at step 83999 : 4.572699164271355\n",
      "Average_loss at step 85999 : 4.738110695719719\n",
      "Average_loss at step 87999 : 4.6825666139125826\n",
      "Average_loss at step 89999 : 4.730355766296387\n",
      "Average_loss at step 91999 : 4.654880146503449\n",
      "Average_loss at step 93999 : 4.693806883096695\n",
      "Average_loss at step 95999 : 4.706434701919556\n",
      "Average_loss at step 97999 : 4.592566548585892\n",
      "Average_loss at step 99999 : 4.525672862529754\n",
      "Average_loss at step 101999 : 4.612213665604592\n",
      "Average_loss at step 103999 : 4.587191364228725\n",
      "Average_loss at step 105999 : 4.59926750433445\n",
      "Average_loss at step 107999 : 4.605999127745628\n",
      "Average_loss at step 109999 : 4.59305643349886\n",
      "Average_loss at step 111999 : 4.562760814547539\n",
      "Average_loss at step 113999 : 4.567513558030129\n",
      "Average_loss at step 115999 : 4.5525305172204975\n",
      "Average_loss at step 117999 : 4.5064506194591525\n",
      "Average_loss at step 119999 : 4.4347663662433625\n",
      "Average_loss at step 121999 : 4.554551405906677\n",
      "Average_loss at step 123999 : 4.60934596836567\n",
      "Average_loss at step 125999 : 4.593172021627426\n",
      "Average_loss at step 127999 : 4.590096567034721\n",
      "Average_loss at step 129999 : 4.5544401433467865\n",
      "Average_loss at step 131999 : 4.515168771505356\n",
      "Average_loss at step 133999 : 4.413066318273544\n",
      "Average_loss at step 135999 : 4.488771807789803\n",
      "Average_loss at step 137999 : 4.522676702737808\n",
      "Average_loss at step 139999 : 4.5309581887722015\n",
      "Average_loss at step 141999 : 4.484772043704987\n",
      "Average_loss at step 143999 : 4.529780826926231\n",
      "Average_loss at step 145999 : 4.483939220845699\n",
      "Average_loss at step 147999 : 4.515843116283417\n",
      "Average_loss at step 149999 : 4.481330386400223\n",
      "Average_loss at step 151999 : 4.503258190870285\n",
      "Average_loss at step 153999 : 4.362984298586845\n",
      "Average_loss at step 155999 : 4.467671084642411\n",
      "Average_loss at step 157999 : 4.500466289043427\n",
      "Average_loss at step 159999 : 4.475838629126549\n",
      "Average_loss at step 161999 : 4.441435071349144\n",
      "Average_loss at step 163999 : 4.458343114614487\n",
      "Average_loss at step 165999 : 4.430018079042434\n",
      "Average_loss at step 167999 : 4.151577945709229\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "saver = tf.train.Saver()\n",
    "initial_step = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    if ckpt is not None:\n",
    "        #print('embed_matrix')\n",
    "        #print(sess.run(skip_gram.embed_matrix))\n",
    "        checkpoint_path = ckpt.model_checkpoint_path\n",
    "        saver.restore(sess,checkpoint_path)\n",
    "        #print('check_point restored')\n",
    "        #print(sess.run(skip_gram.embed_matrix))\n",
    "    initial_step = skip_gram.global_step.eval()\n",
    "    total_loss=0\n",
    "    for i in range(initial_step,initial_step+NUM_TRAIN_STEPS): \n",
    "        example_batch = next(example_batch_generator)\n",
    "        center_word_batch = example_batch[:,0]\n",
    "        target_word_batch = example_batch[:,1]\n",
    "        center_word_batch = center_word_batch.reshape(center_word_batch.shape[0])\n",
    "        target_word_batch = target_word_batch.reshape((target_word_batch.shape[0],1))\n",
    "        center_word_batch = center_word_batch.astype(np.int32)\n",
    "        target_word_batch = target_word_batch.astype(np.int32)\n",
    "\n",
    "        feed_dict = {\n",
    "            skip_gram.center_words_batch : center_word_batch,\n",
    "            skip_gram.target_words_batch : target_word_batch\n",
    "        }\n",
    "        loss,_,summary = sess.run([skip_gram.loss,skip_gram.optimizer,skip_gram.optimizer],feed_dict=feed_dict)\n",
    "        total_loss+= loss\n",
    "        if (i+1)%SKIP_STEP ==0:\n",
    "            \n",
    "            print(\"Average_loss at step {} : {}\".format(i,total_loss/SKIP_STEP))\n",
    "            saver.save(sess,'checkpoints/skip-gram',global_step=i)\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/skip-gram-167999\n"
     ]
    }
   ],
   "source": [
    "#take out word_embedding weight\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    if ckpt is not None:\n",
    "        checkpoint_path = ckpt.model_checkpoint_path\n",
    "        saver.restore(sess,checkpoint_path)\n",
    "    embed_mat = sess.run(skip_gram.embed_matrix)\n",
    "    #print(embed_mat.shape)\n",
    "    #print(embed_mat.dtype)\n",
    "    #print(type(embed_mat))\n",
    "    embedding_var = tf.Variable(embed_mat[:1000],name='embedding')\n",
    "    sess.run(embedding_var.initializer)\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    embedding.metadata_path = '/processed/vocab_1000.tsv'\n",
    "\n",
    "    projector.visualize_embeddings(writer,config)\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess,'processed/model3.ckpt',1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
